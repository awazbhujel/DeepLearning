{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**1.AlexNet Architecture**"
      ],
      "metadata": {
        "id": "eXTprShvti2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is AlexNet?**\n",
        "\n",
        "AlexNet is a Convolutional Neural Network (CNN) architecture that won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. It was designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. The network was notable for its deep architecture and use of GPUs for training, which set new benchmarks for image classification performance.\n",
        "\n",
        "**Key Features of AlexNet Architecture:**\n",
        "\n",
        "1. Layers: AlexNet consists of 5 convolutional layers followed by 3 fully connected layers.\n",
        "2. Activation Function: Uses ReLU (Rectified Linear Unit) activation function.\n",
        "Pooling: Employs max pooling layers.\n",
        "3. Normalization: Introduces local response normalization (LRN).\n",
        "4. Dropout: Uses dropout in the fully connected layers to reduce overfitting.\n",
        "Training:\n",
        "5. Data Augmentation: Uses extensive data augmentation techniques.\n",
        "6. GPU Utilization: Utilizes two GPUs to parallelize training."
      ],
      "metadata": {
        "id": "PiLbYMRTrNxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Where to Use AlexNet**\n",
        "1. Image Classification: Suitable for image classification tasks, especially with datasets that have a large number of diverse categories.\n",
        "2. Feature Extraction: Can be used as a feature extractor for other vision tasks by leveraging the pretrained weights on ImageNet.\n",
        "3. Transfer Learning: Ideal for transfer learning on similar image classification tasks."
      ],
      "metadata": {
        "id": "n6vodg2mretI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AlexNet Architecture in Keras**"
      ],
      "metadata": {
        "id": "sZIYfBFSrms0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "eAfNECsqrjrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define alexnet architecture\n",
        "model = Sequential()\n",
        "\n",
        "#1st Convolutional Layer\n",
        "model.add(Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(227, 227, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "#2nd Convolutional Layer\n",
        "model.add(Conv2D(256, (5, 5), activation='relu',padding=\"same\"))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "#3rd convolutional layer\n",
        "model.add(Conv2D(384, (3, 3), activation='relu',padding=\"Same\"))\n",
        "\n",
        "#4th convolutional layer\n",
        "model.add(Conv2D(384, (3, 3), activation='relu',padding=\"Same\"))\n",
        "\n",
        "#5th convolutional layer\n",
        "model.add(Conv2D(256, (3, 3), activation='relu',padding=\"Same\"))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "model.add(BatchNormalization())"
      ],
      "metadata": {
        "id": "me0Mx9tTrjnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# 1st Fully Connected Layer\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# 2nd Fully Connected Layer\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(1000, activation='softmax'))"
      ],
      "metadata": {
        "id": "5eHuDgeirjk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aYBsXHorjhw",
        "outputId": "eabd4017-765d-4e5e-9623-b934fd8ff9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 55, 55, 96)        34944     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 27, 27, 96)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 27, 27, 96)        384       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 27, 27, 256)       614656    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 13, 13, 256)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 13, 13, 256)       1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 13, 13, 384)       885120    \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 13, 13, 256)       884992    \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 6, 6, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 6, 6, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 9216)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4096)              37752832  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1000)              4097000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62380776 (237.96 MB)\n",
            "Trainable params: 62379560 (237.96 MB)\n",
            "Non-trainable params: 1216 (4.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Evaluation**"
      ],
      "metadata": {
        "id": "mnX6sxe3t0qM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data generators for training and validation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'path_to_train_data',\n",
        "    target_size=(227, 227),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    'path_to_validation_data',\n",
        "    target_size=(227, 227),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50\n",
        ")\n"
      ],
      "metadata": {
        "id": "12DTGALstv2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Pretrained Models**"
      ],
      "metadata": {
        "id": "Caue56oLGcFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.ResNet50**"
      ],
      "metadata": {
        "id": "qZFhbgK9F-mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the ResNet50 model with pretrained weights from ImageNet\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom top layers for our specific task\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(10, activation='softmax')(x)  # Assume 10 classes\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Prepare the data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'path_to_train_data',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    'path_to_validation_data',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print(f'ResNet50 Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "eLzxc5CktvzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. VGG16**"
      ],
      "metadata": {
        "id": "7scmbJ7nGUoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the VGG16 model with pretrained weights from ImageNet\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom top layers for our specific task\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(10, activation='softmax')(x)  # Assume 10 classes\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Prepare the data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'path_to_train_data',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    'path_to_validation_data',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print(f'VGG16 Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "vmtLa7eStvxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Xception**"
      ],
      "metadata": {
        "id": "r5df4NOkGmTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the Xception model with pretrained weights from ImageNet\n",
        "base_model = Xception(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "# Add custom top layers for our specific task\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(10, activation='softmax')(x)  # Assume 10 classes\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Prepare the data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'path_to_train_data',\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    'path_to_validation_data',\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print(f'Xception Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "Kac8xOqarjfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When to Use and When Not to Use Each Model**\n",
        "**1.ResNet50**\n",
        "\n",
        "**When to Use:**\n",
        "\n",
        "1. Image Classification: Suitable for various image classification tasks due to its depth and ability to learn complex features.\n",
        "2. Transfer Learning: Effective as a feature extractor for other tasks like object detection and image segmentation, especially when pretrained on large datasets like ImageNet.\n",
        "3. Deeper Networks Needed: When you need a very deep network that can still be trained efficiently without facing the vanishing gradient problem.\n",
        "\n",
        "**When Not to Use:**\n",
        "\n",
        "1. Resource Constraints: If computational resources are limited (e.g., on edge devices or mobile phones), as ResNet50 can be computationally expensive and memory-intensive.\n",
        "2. Simplicity Required: For simpler tasks where a less complex model can achieve similar performance with less computational overhead.\n",
        "3. Real-time Applications: If low latency is critical, as ResNet50 may not meet the performance requirements for real-time processing.\n",
        "\n",
        "**2. VGG16**\n",
        "\n",
        "When to Use:\n",
        "\n",
        "1. Benchmarking: VGG16 is a well-established model often used for benchmarking new techniques and models.\n",
        "2. Feature Extraction: Useful for extracting features from images due to its straightforward and consistent architecture.\n",
        "3. Transfer Learning: Effective for transfer learning, especially for tasks requiring high-quality feature extraction.\n",
        "\n",
        "When Not to Use:\n",
        "\n",
        "1. Efficiency: VGG16 is less efficient compared to modern architectures like EfficientNet and ResNet. It has a large number of parameters, leading to high computational and memory costs.\n",
        "2. Depth Limitations: It is relatively shallow compared to more recent architectures like ResNet and may not perform as well on very complex tasks.\n",
        "3. Real-time Applications: Similar to ResNet50, VGG16 may not be suitable for applications requiring low latency due to its computational demands.\n",
        "\n",
        "**3.Xception**\n",
        "\n",
        "When to Use:\n",
        "\n",
        "1. Advanced Applications: Suitable for advanced applications requiring efficient computation with high performance, such as image classification, segmentation, and detection.\n",
        "2. Transfer Learning: Excellent for transfer learning due to its depth and efficient architecture, providing good feature representations.\n",
        "3. Comparative Studies: Useful for comparing with other advanced architectures like Inception and EfficientNet, as it combines depthwise separable convolutions for improved efficiency.\n",
        "\n",
        "\n",
        "When Not to Use:\n",
        "\n",
        "1. Resource Constraints: While more efficient than VGG16, Xception can still be resource-intensive and may not be ideal for very limited environments.\n",
        "2. Simpler Tasks: For simpler tasks where the depth and complexity of Xception are unnecessary, leading to overfitting and wasted computational resources.\n",
        "3. Real-time Applications: Although more efficient, it may still be too heavy for real-time applications that require minimal latency."
      ],
      "metadata": {
        "id": "eFEwkNu4G66v"
      }
    }
  ]
}